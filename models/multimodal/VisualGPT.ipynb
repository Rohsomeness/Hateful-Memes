{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VisualGPT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNInrICfO94K28ubLWFOPIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1f70b52d87bd4509a7d7d292c8320c43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bf137f37007f49c5abf3911a3cf0efdd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8a03edb8d0e8436d97b39115c11207e8","IPY_MODEL_9b4d7a4f7d1c41d9965b4f65f4625bf3"]}},"bf137f37007f49c5abf3911a3cf0efdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a03edb8d0e8436d97b39115c11207e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3c68d4eb9eed489685b43a1e1beaa101","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1042301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1042301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7851c73a1e240cbba2b2598e41b188a"}},"9b4d7a4f7d1c41d9965b4f65f4625bf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_189998110fce448c93a4bdef347e8fa7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.04M/1.04M [00:00&lt;00:00, 1.85MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d889d8900874afcb4488ff3e0f21d77"}},"3c68d4eb9eed489685b43a1e1beaa101":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d7851c73a1e240cbba2b2598e41b188a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"189998110fce448c93a4bdef347e8fa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d889d8900874afcb4488ff3e0f21d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d650128970f4444fbf00b1421752cb85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a7477923ec6b43d38563ce62f4910b6d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1451128d4f44078ae0eb03d8fa6c8e9","IPY_MODEL_79c33af98b8246cc94590a05b3c6479d"]}},"a7477923ec6b43d38563ce62f4910b6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1451128d4f44078ae0eb03d8fa6c8e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c611003150e842818ed58288dbb539bf","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4b6897492e59482385cb258bafbbf607"}},"79c33af98b8246cc94590a05b3c6479d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85afdc044fdd4d4fbba8667c0f903bf3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 1.89MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ab3aad1ca04450dac31428de2fe9153"}},"c611003150e842818ed58288dbb539bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4b6897492e59482385cb258bafbbf607":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85afdc044fdd4d4fbba8667c0f903bf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3ab3aad1ca04450dac31428de2fe9153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f0f7f5e854a4cdbaedc7e64e8e55a6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9094f9ba4f404d2890eea5aa583292c0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d9a81ede383d466d980efa4bed8a973c","IPY_MODEL_227717146daa4ef4b5c1e88478b21c03"]}},"9094f9ba4f404d2890eea5aa583292c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d9a81ede383d466d980efa4bed8a973c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_82b083239afc409d8297fca030e06da6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1355256,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355256,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbb03d3c00d34c51a853ffce81588f75"}},"227717146daa4ef4b5c1e88478b21c03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7ae5a95e15744adc8acf6e796a5c0b7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 6.93MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_313ca38fd3b54379a56bd2d61229e9f0"}},"82b083239afc409d8297fca030e06da6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dbb03d3c00d34c51a853ffce81588f75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ae5a95e15744adc8acf6e796a5c0b7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"313ca38fd3b54379a56bd2d61229e9f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"xTGknb7covg9"},"source":["# Download the GPT-2 pretrained weights"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTIry55vok25","executionInfo":{"status":"ok","timestamp":1618894610192,"user_tz":240,"elapsed":9960,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"31efbd45-7467-4d39-afde-08243fb47493"},"source":["!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  522M  100  522M    0     0  55.7M      0  0:00:09  0:00:09 --:--:-- 53.4M\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CzldOXOWo4gN"},"source":["# Implementation of VisualGPT "]},{"cell_type":"markdown","metadata":{"id":"Zp18V2d0o_p3"},"source":["## `models/transformer/utils.py`"]},{"cell_type":"code","metadata":{"id":"BTilU4UZo31y"},"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","def position_embedding(input, d_model):\n","    input = input.view(-1, 1)\n","    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n","    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n","    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n","\n","    out = torch.zeros((input.shape[0], d_model), device=input.device)\n","    out[:, ::2] = sin\n","    out[:, 1::2] = cos\n","    return out\n","\n","\n","def sinusoid_encoding_table(max_len, d_model, padding_idx=None):\n","    pos = torch.arange(max_len, dtype=torch.float32)\n","    out = position_embedding(pos, d_model)\n","\n","    if padding_idx is not None:\n","        out[padding_idx] = 0\n","    return out\n","\n","\n","class PositionWiseFeedForward(nn.Module):\n","    '''\n","    Position-wise feed forward layer\n","    '''\n","\n","    def __init__(self, d_model=512, d_ff=2048, dropout=.1, identity_map_reordering=False):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.identity_map_reordering = identity_map_reordering\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.dropout_2 = nn.Dropout(p=dropout)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, input):\n","        if self.identity_map_reordering:\n","            out = self.layer_norm(input)\n","            out = self.fc2(self.dropout_2(F.relu(self.fc1(out))))\n","            out = input + self.dropout(torch.relu(out))\n","        else:\n","            out = self.fc2(self.dropout_2(F.relu(self.fc1(input))))\n","            out = self.dropout(out)\n","            out = self.layer_norm(input + out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OW3Gd5LUpTPB"},"source":["## `utils/typing.py`"]},{"cell_type":"code","metadata":{"id":"EwADKjYZpSyD"},"source":["from typing import Union, Sequence, Tuple\n","import torch\n","\n","TensorOrSequence = Union[Sequence[torch.Tensor], torch.Tensor]\n","TensorOrNone = Union[torch.Tensor, None]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H7vgRi1Tpi_D"},"source":["## `models/container.py`"]},{"cell_type":"code","metadata":{"id":"otVRWRP6oyhY"},"source":["from contextlib import contextmanager\n","from torch import nn\n","\n","\n","class Module(nn.Module):\n","    def __init__(self):\n","        super(Module, self).__init__()\n","        self._is_stateful = False\n","        self._state_names = []\n","        self._state_defaults = dict()\n","\n","    def register_state(self, name: str, default: TensorOrNone):\n","        self._state_names.append(name)\n","        if default is None:\n","            self._state_defaults[name] = None\n","        else:\n","            self._state_defaults[name] = default.clone().detach()\n","        self.register_buffer(name, default)\n","\n","    def states(self):\n","        for name in self._state_names:\n","            yield self._buffers[name]\n","        for m in self.children():\n","            if isinstance(m, Module):\n","                yield from m.states()\n","\n","    def apply_to_states(self, fn):\n","        for name in self._state_names:\n","            self._buffers[name] = fn(self._buffers[name])\n","        for m in self.children():\n","            if isinstance(m, Module):\n","                m.apply_to_states(fn)\n","\n","    def _init_states(self, batch_size: int):\n","        for name in self._state_names:\n","            if self._state_defaults[name] is None:\n","                self._buffers[name] = None\n","            else:\n","                self._buffers[name] = self._state_defaults[name].clone().detach().to(self._buffers[name].device)\n","                self._buffers[name] = self._buffers[name].unsqueeze(0)\n","                self._buffers[name] = self._buffers[name].expand([batch_size, ] + list(self._buffers[name].shape[1:]))\n","                self._buffers[name] = self._buffers[name].contiguous()\n","\n","    def _reset_states(self):\n","        for name in self._state_names:\n","            if self._state_defaults[name] is None:\n","                self._buffers[name] = None\n","            else:\n","                self._buffers[name] = self._state_defaults[name].clone().detach().to(self._buffers[name].device)\n","\n","    def enable_statefulness(self, batch_size: int):\n","        for m in self.children():\n","            if isinstance(m, Module):\n","                m.enable_statefulness(batch_size)\n","        self._init_states(batch_size)\n","        self._is_stateful = True\n","        # self._is_stateful = False\n","\n","    def disable_statefulness(self):\n","        for m in self.children():\n","            if isinstance(m, Module):\n","                m.disable_statefulness()\n","        self._reset_states()\n","        self._is_stateful = False\n","\n","    @contextmanager\n","    def statefulness(self, batch_size: int):\n","        self.enable_statefulness(batch_size)\n","        try:\n","            yield\n","        finally:\n","            self.disable_statefulness()\n","\n","\n","class ModuleList(nn.ModuleList, Module):\n","    pass\n","\n","\n","class ModuleDict(nn.ModuleDict, Module):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc5rlOJ8p3H-"},"source":["## `models/transformers/attention.py`"]},{"cell_type":"code","metadata":{"id":"H3jeksVFpin_"},"source":["import numpy as np\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","\n","class Conv1D(nn.Module):\n","    def __init__(self, nf, nx):\n","        super(Conv1D, self).__init__()\n","        self.nf = nf\n","        w = torch.empty(nx, nf)\n","        # nn.init.normal_(w, std=0.02)\n","        nn.init.xavier_uniform_(w)\n","        self.weight = Parameter(w)\n","        self.bias = Parameter(torch.zeros(nf))\n","\n","    def forward(self, x):\n","        size_out = x.size()[:-1] + (self.nf,)\n","        # test = x.contiguous().view(-1, x.size(-1))\n","        x = torch.addmm(self.bias, x.contiguous().view(-1, x.size(-1)), self.weight.transpose(1,0))\n","\n","        x = x.view(*size_out)\n","        return x\n","\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    '''\n","    Scaled dot-product attention\n","    '''\n","\n","    def __init__(self, d_model, d_k, d_v, h):\n","        '''\n","        :param d_model: Output dimensionality of the model\n","        :param d_k: Dimensionality of queries and keys\n","        :param d_v: Dimensionality of values\n","        :param h: Number of heads\n","        '''\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.fc_q = nn.Linear(d_model, h * d_k)\n","        self.fc_k = nn.Linear(d_model, h * d_k)\n","        self.fc_v = nn.Linear(d_model, h * d_v)\n","\n","        self.fc_o = nn.Linear(h * d_v, d_model)\n","\n","\n","\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.h = h\n","\n","        self.c_attn_query = Conv1D(768 , 768)\n","        self.c_attn_key = Conv1D(768, 768)\n","        self.c_attn_value = Conv1D(768, 768)\n","\n","        self.split_size = 768\n","        self.n_head = 12\n","        self.init_weights()\n","\n","        self.flag = None\n","\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform_(self.fc_q.weight)\n","        nn.init.xavier_uniform_(self.fc_k.weight)\n","        nn.init.xavier_uniform_(self.fc_v.weight)\n","\n","\n","        nn.init.xavier_uniform_(self.fc_o.weight)\n","        nn.init.constant_(self.fc_q.bias, 0)\n","        nn.init.constant_(self.fc_k.bias, 0)\n","        nn.init.constant_(self.fc_v.bias, 0)\n","\n","\n","\n","        nn.init.constant_(self.fc_o.bias, 0)\n","\n","        # nn.init.xavier_uniform_(self.c_attn_query.weight)\n","        # nn.init.xavier_uniform_(self.c_attn_key.weight)\n","        # nn.init.xavier_uniform_(self.c_attn_value.weight)\n","        #\n","        # nn.init.constant_(self.c_attn_query.bias,0)\n","        # nn.init.constant_(self.c_attn_key.bias,0)\n","        # nn.init.constant_(self.c_attn_value.bias,0 )\n","\n","\n","    def split_heads(self, x, k=False):\n","        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n","        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n","        if k:\n","            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n","        else:\n","            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n","\n","\n","    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n","        '''\n","        Computes\n","        :param queries: Queries (b_s, nq, d_model)\n","        :param keys: Keys (b_s, nk, d_model)\n","        :param values: Values (b_s, nk, d_model)\n","        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n","        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n","        :return:\n","        '''\n","\n","        b_s, nq = queries.shape[:2]\n","        nk = keys.shape[1]\n","\n","\n","\n","\n","        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n","        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n","        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n","\n","\n","        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n","        if attention_weights is not None:\n","            att = att * attention_weights\n","        if attention_mask is not None:\n","            att = att.masked_fill(attention_mask, -np.inf)\n","        att = torch.softmax(att, -1)\n","        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n","        out = self.fc_o(out)  # (b_s, nq, d_model)\n","        return out\n","\n","\n","class ScaledDotProductAttentionMemory(nn.Module):\n","    '''\n","    Scaled dot-product attention with memory\n","    '''\n","\n","    def __init__(self, d_model, d_k, d_v, h, m):\n","        '''\n","        :param d_model: Output dimensionality of the model\n","        :param d_k: Dimensionality of queries and keys\n","        :param d_v: Dimensionality of values\n","        :param h: Number of heads\n","        :param m: Number of memory slots\n","        '''\n","        super(ScaledDotProductAttentionMemory, self).__init__()\n","        self.fc_q = nn.Linear(d_model, h * d_k)\n","        self.fc_k = nn.Linear(d_model, h * d_k)\n","        self.fc_v = nn.Linear(d_model, h * d_v)\n","        self.fc_o = nn.Linear(h * d_v, d_model)\n","        # self.m_k = nn.Parameter(torch.FloatTensor(1, m, h * d_k))\n","        # self.m_v = nn.Parameter(torch.FloatTensor(1, m, h * d_v))\n","\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.h = h\n","\n","        # self.m = m\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform_(self.fc_q.weight)\n","        nn.init.xavier_uniform_(self.fc_k.weight)\n","        nn.init.xavier_uniform_(self.fc_v.weight)\n","        nn.init.xavier_uniform_(self.fc_o.weight)\n","        # nn.init.normal_(self.m_k, 0, 1 / self.d_k)\n","        # nn.init.normal_(self.m_v, 0, 1 / self.m)\n","        nn.init.constant_(self.fc_q.bias, 0)\n","        nn.init.constant_(self.fc_k.bias, 0)\n","        nn.init.constant_(self.fc_v.bias, 0)\n","        nn.init.constant_(self.fc_o.bias, 0)\n","\n","    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n","        '''\n","        Computes\n","        :param queries: Queries (b_s, nq, d_model)\n","        :param keys: Keys (b_s, nk, d_model)\n","        :param values: Values (b_s, nk, d_model)\n","        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n","        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n","        :return:\n","        '''\n","        b_s, nq = queries.shape[:2]\n","        nk = keys.shape[1]\n","\n","        # m_k = np.sqrt(self.d_k) * self.m_k.expand(b_s, self.m, self.h * self.d_k)\n","        # m_v = np.sqrt(self.m) * self.m_v.expand(b_s, self.m, self.h * self.d_v)\n","\n","        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n","        k = self.fc_k(keys).view(b_s, nk , self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n","        v = self.fc_v(values).view(b_s, nk , self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n","\n","        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n","        if attention_weights is not None:\n","            att = torch.cat([att[:, :, :, :nk] * attention_weights, att[:, :, :, nk:]], -1)\n","        if attention_mask is not None:\n","            att[:, :, :, :nk] = att[:, :, :, :nk].masked_fill(attention_mask, -np.inf)\n","        att = torch.softmax(att, -1)\n","        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n","        out = self.fc_o(out)  # (b_s, nq, d_model)\n","        return out\n","\n","\n","class MultiHeadAttention(Module):\n","    '''\n","    Multi-head attention layer with Dropout and Layer Normalization.\n","    '''\n","\n","    def __init__(self, d_model, d_k, d_v, h, dropout=.1, identity_map_reordering=False, can_be_stateful=False,\n","                 attention_module=None, attention_module_kwargs=None):\n","        super(MultiHeadAttention, self).__init__()\n","        self.identity_map_reordering = identity_map_reordering\n","        if attention_module is not None:\n","            if attention_module_kwargs is not None:\n","                self.attention = attention_module(d_model=d_model, d_k=d_k, d_v=d_v, h=h, **attention_module_kwargs)\n","            else:\n","                self.attention = attention_module(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n","        else:\n","            self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","        self.can_be_stateful = can_be_stateful\n","        if self.can_be_stateful:\n","            self.register_state('running_keys', torch.zeros((0, d_model)))\n","            self.register_state('running_values', torch.zeros((0, d_model)))\n","\n","    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n","        if self.can_be_stateful and self._is_stateful:\n","\n","\n","            self.running_keys = torch.cat([self.running_keys, keys], 1)\n","            keys = self.running_keys\n","\n","\n","            self.running_values = torch.cat([self.running_values, values], 1)\n","            values = self.running_values\n","\n","        if self.identity_map_reordering:\n","            q_norm = self.layer_norm(queries)\n","            k_norm = self.layer_norm(keys)\n","            v_norm = self.layer_norm(values)\n","            out = self.attention(q_norm, k_norm, v_norm, attention_mask, attention_weights)\n","            out = queries + self.dropout(torch.relu(out))\n","        else:\n","            out = self.attention(queries, keys, values, attention_mask, attention_weights)\n","            out = self.dropout(out)\n","            out = self.layer_norm(queries + out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVz8Yq3Pp_Wx"},"source":["## `models/encoders.py` (for encoding image)"]},{"cell_type":"code","metadata":{"id":"T87aUDgZp-rS"},"source":["import math\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model=768, d_k=64, d_v=64, h=12, d_ff=2048, dropout=.1, identity_map_reordering=False,\n","                 attention_module=None, attention_module_kwargs=None):\n","        super(EncoderLayer, self).__init__()\n","        self.identity_map_reordering = identity_map_reordering\n","        self.mhatt = MultiHeadAttention(d_model, d_k, d_v, h, dropout, identity_map_reordering=identity_map_reordering,\n","                                        attention_module=attention_module,\n","                                        attention_module_kwargs=attention_module_kwargs)\n","        self.pwff = PositionWiseFeedForward(d_model, d_ff, dropout, identity_map_reordering=identity_map_reordering)\n","\n","    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n","        att = self.mhatt(queries, keys, values, attention_mask, attention_weights)\n","        ff = self.pwff(att)\n","        return ff\n","\n","\n","class MultiLevelEncoder(nn.Module):\n","    def __init__(self, N, padding_idx, d_model=768, d_k=64, d_v=64, h=12, d_ff=2048, dropout=.1,\n","                 identity_map_reordering=False, attention_module=None, attention_module_kwargs=None):\n","        super(MultiLevelEncoder, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = dropout\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, d_k, d_v, h, d_ff, dropout,\n","                                                  identity_map_reordering=identity_map_reordering,\n","                                                  attention_module=attention_module,\n","                                                  attention_module_kwargs=attention_module_kwargs)\n","                                     for _ in range(N)])\n","        self.padding_idx = padding_idx\n","\n","    def forward(self, input, attention_weights=None):\n","        # input (b_s, seq_len, d_in)\n","        attention_mask = (torch.sum(input, -1) == self.padding_idx).unsqueeze(1).unsqueeze(1) # (b_s, 1, 1, seq_len)\n","\n","        outs = []\n","        out = input\n","        for l in self.layers:\n","            out = l(out, out, out, attention_mask, attention_weights)\n","            outs.append(out.unsqueeze(1))\n","\n","        outs = torch.cat(outs, 1)\n","        return outs, attention_mask\n","\n","def gelu(x):\n","    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","\n","class VisualEncoder(MultiLevelEncoder):\n","    def __init__(self, N, padding_idx, d_in=2048, **kwargs):\n","        super(VisualEncoder, self).__init__(N, padding_idx, **kwargs)\n","        self.fc = nn.Linear(d_in, self.d_model)\n","        self.dropout = nn.Dropout(p=self.dropout)\n","        self.layer_norm = nn.LayerNorm(self.d_model)\n","\n","    def forward(self, input, attention_weights=None):\n","        # out = F.relu(self.fc(input))\n","\n","\n","        out = gelu(self.fc(input))\n","        out = self.dropout(out)\n","        out = self.layer_norm(out)\n","\n","        return super(VisualEncoder, self).forward(out, attention_weights=attention_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8W_GFs83T2S","executionInfo":{"status":"ok","timestamp":1618898444312,"user_tz":240,"elapsed":930,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"9914f670-3611-41af-d3de-62b731d8e0ac"},"source":["VisualEncoder(3, 0, 512)(torch.randn(1, 1, 512))[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 1, 768])"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"nw2-Rh9aqV0L"},"source":["## `models/transformer/config.py` (Configs for GPT2)"]},{"cell_type":"code","metadata":{"id":"8YTPRemDqTJo"},"source":["'''\n","    code by TaeHwan Jung(@graykode)\n","    Original Paper and repository here : https://github.com/openai/gpt-2\n","    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n","'''\n","class GPT2Config(object):\n","    def __init__(\n","            self,\n","            vocab_size_or_config_json_file=50257,\n","            n_positions=1024,\n","            n_ctx=60,\n","            n_embd=768,\n","            n_layer=12,\n","            n_head=12,\n","            layer_norm_epsilon=1e-5,\n","            initializer_range=0.02,\n","            attn_pdrop=0.1,\n","            resid_pdrop = 0.1,\n","            \n","\n","\n","    ):\n","        self.vocab_size = vocab_size_or_config_json_file\n","        self.n_ctx = n_ctx\n","        self.n_positions = n_positions\n","        self.n_embd = n_embd\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.layer_norm_epsilon = layer_norm_epsilon\n","        self.initializer_range = initializer_range\n","        self.attn_pdrop = attn_pdrop\n","        self.resid_pdrop = resid_pdrop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7DSCc3_qwx6"},"source":["## `models/transformer/load_gptmodel.py`"]},{"cell_type":"code","metadata":{"id":"WR_aUIIqqopu"},"source":["'''\n","    code by TaeHwan Jung(@graykode)\n","    Original Paper and repository here : https://github.com/openai/gpt-2\n","    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n","'''\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","def load_weight(model, state_dict):\n","    old_keys = []\n","    new_keys = []\n","    for key in state_dict.keys():\n","\n","\n","        new_key = None\n","        if key.endswith(\".g\"):\n","            new_key = key[:-2] + \".weight\"\n","        elif key.endswith(\".b\"):\n","            new_key = key[:-2] + \".bias\"\n","        elif key.endswith(\".w\"):\n","            new_key = key[:-2] + \".weight\"\n","        if new_key:\n","            old_keys.append(key)\n","            new_keys.append(new_key)\n","    for old_key, new_key in zip(old_keys, new_keys):\n","        state_dict[new_key] = state_dict.pop(old_key)\n","\n","    missing_keys = []\n","    unexpected_keys = []\n","    error_msgs = []\n","    # copy state_dict so _load_from_state_dict can modify it\n","    metadata = getattr(state_dict, \"_metadata\", None)\n","    state_dict = state_dict.copy()\n","    if metadata is not None:\n","        state_dict._metadata = metadata\n","\n","    def load(module, prefix=\"\"):\n","        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n","        module._load_from_state_dict(\n","            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n","        )\n","        for name, child in module._modules.items():\n","            if child is not None:\n","                load(child, prefix + name + \".\")\n","\n","    start_model = model\n","    if hasattr(model, \"transformer\") and all(not s.startswith('transformer.') for s in state_dict.keys()):\n","        start_model = model.transformer\n","    load(start_model, prefix=\"\")\n","\n","    # Make sure we are still sharing the output and input embeddings after loading weights\n","    # model.set_tied() <-- only needed if we are using GPT2MLMHead\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObXLDV3Pq8OP"},"source":["## `models/transformer/gpt_decoder_visualGPT.py` (implements GPT2)"]},{"cell_type":"code","metadata":{"id":"TlHEsUbhq5n1"},"source":["'''\n","    code by TaeHwan Jung(@graykode)\n","    Original Paper and repository here : https://github.com/openai/gpt-2\n","    GPT2 Pytorch Model : https://github.com/huggingface/pytorch-pretrained-BERT\n","'''\n","import copy\n","import torch\n","import math\n","import torch.nn as nn\n","from torch.nn.parameter import Parameter\n","import numpy as np\n","from torch.nn import functional as F\n","\n","def gelu(x):\n","    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-12):\n","        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n","        \"\"\"\n","        super(LayerNorm, self).__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.bias = nn.Parameter(torch.zeros(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, x):\n","        u = x.mean(-1, keepdim=True)\n","        s = (x - u).pow(2).mean(-1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.weight * x + self.bias\n","\n","class Conv1D(nn.Module):\n","    def __init__(self, nf, nx):\n","        super(Conv1D, self).__init__()\n","        self.nf = nf\n","        w = torch.empty(nx, nf)\n","        nn.init.normal_(w, std=0.02)\n","        self.weight = Parameter(w)\n","        self.bias = Parameter(torch.zeros(nf))\n","\n","    def forward(self, x):\n","        size_out = x.size()[:-1] + (self.nf,)\n","        test = x.contiguous().view(-1, x.size(-1))\n","        x = torch.addmm(self.bias, x.contiguous().view(-1, x.size(-1)), self.weight)\n","\n","        x = x.view(*size_out)\n","        return x\n","\n","\n","class Attention(Module):\n","    def __init__(self, nx, n_ctx, config, scale=False,can_be_stateful=False):\n","        super(Attention, self).__init__()\n","\n","        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n","        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n","        assert n_state % config.n_head == 0\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n","        self.n_head = config.n_head\n","        self.split_size = n_state\n","        self.scale = scale\n","        self.c_attn = Conv1D(n_state * 3, nx)\n","        self.c_proj = Conv1D(n_state, nx)\n","        self.can_be_stateful = can_be_stateful\n","        self.attn_pdrop = nn.Dropout(config.attn_pdrop)\n","\n","        if self.can_be_stateful:\n","            self.register_state('running_keys', torch.zeros((12,0, 64)))\n","            self.register_state('running_values', torch.zeros((12,0, 64)))\n","\n","\n","    def _attn(self, q, k, v,mask_self_attention):\n","\n","        w = torch.matmul(q, k)\n","        if self.scale:\n","            w = w / math.sqrt(v.size(-1))\n","\n","        if mask_self_attention is not None:\n","\n","\n","            w = w.masked_fill(mask_self_attention, -10000.0)\n","            # w[:,:,:,:nk] = w[:,:,:,:nk].masked_fill(mask_self_attention, -1e7)\n","        # nd, ns = w.size(-2), w.size(-1)\n","        # b = self.bias[:, :, ns-nd:ns, :ns]\n","\n","        # w = w * b - 1e10 * (1 - b)\n","        w = nn.Softmax(dim=-1)(w)\n","        self.w = self.attn_pdrop(w)\n","        return torch.matmul(w, v)\n","\n","    def merge_heads(self, x):\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n","        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n","\n","    def split_heads(self, x, k=False):\n","        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n","        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n","        if k:\n","            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n","        else:\n","            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n","\n","    def forward(self, x, layer_past=None,mask_self_attention=None):\n","        x = self.c_attn(x)\n","        query, key, value = x.split(self.split_size, dim=2)\n","        query = self.split_heads(query)\n","        key = self.split_heads(key, k=True)\n","        value = self.split_heads(value)\n","\n","        if self.can_be_stateful and self._is_stateful:\n","            self.running_keys = torch.cat([self.running_keys, key.transpose(-2,-1)],-2)\n","            key = self.running_keys.transpose(-2,-1)\n","\n","            self.running_values = torch.cat([self.running_values, value], -2)\n","            value = self.running_values\n","        # if layer_past is not None:\n","        #     past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n","        #     key = torch.cat((past_key, key), dim=-1)\n","        #     value = torch.cat((past_value, value), dim=-2)\n","\n","        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n","        a = self._attn(query, key, value,mask_self_attention)\n","        a = self.merge_heads(a)\n","        a = self.c_proj(a)\n","\n","\n","        return a, present\n","\n","\n","class Enc_Dec_Attention(Module):\n","    def __init__(self, nx, n_ctx, config, scale=False):\n","        super(Enc_Dec_Attention, self).__init__()\n","        n_state = nx = 768\n","        n_ctx = 60\n","        scale = True\n","        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n","        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n","        assert n_state % 12 == 0\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n","        self.n_head = 12\n","        self.split_size = n_state\n","        self.scale = scale\n","        self.c_attn = Conv1D(n_state * 3, nx)\n","        self.c_proj = Conv1D(n_state, nx)\n","\n","        self.fc_q = nn.Linear(n_state, 64 * 12)\n","        self.fc_k = nn.Linear(n_state, 64 * 12)\n","        self.fc_v = nn.Linear(n_state, 64 * 12)\n","\n","        self.attn_dropout = nn.Dropout(0.2)\n","\n","        self.init_weights()\n","\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform_(self.fc_q.weight)\n","        nn.init.xavier_uniform_(self.fc_k.weight)\n","        nn.init.xavier_uniform_(self.fc_v.weight)\n","\n","        nn.init.constant_(self.fc_q.bias, 0)\n","        nn.init.constant_(self.fc_k.bias, 0)\n","        nn.init.constant_(self.fc_v.bias, 0)\n","        # nn.init.xavier_uniform_(self.fc_o.weight)\n","\n","\n","\n","    def _attn(self, q, k, v,enc_dec_attention):\n","        nk = k.shape[-1]\n","        w = torch.matmul(q, k)\n","        if self.scale:\n","            w = w / math.sqrt(v.size(-1))\n","        nd, ns = w.size(-2), w.size(-1)\n","        b = self.bias[:, :, ns-nd:ns, :ns]\n","        if enc_dec_attention is not None:\n","            w = w.masked_fill(enc_dec_attention, -10000.0)\n","            # w[:, :, ns-nd:ns, :ns] = w[:, :, ns-nd:ns, :ns].masked_fill(enc_dec_attention, -1e10)\n","\n","        # w = w*enc_dec_attention\n","\n","        # w = w * b - 1e10 * (1 - b)\n","        w = nn.Softmax(dim=-1)(w)\n","        w = self.attn_dropout(w)\n","        return torch.matmul(w, v)\n","\n","    def merge_heads(self, x):\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n","        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n","\n","    def split_heads(self, x, k=False):\n","        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n","        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n","        if k:\n","            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n","        else:\n","            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n","\n","    def forward(self, x, layer_past=None,encoder_output=None, mask_encoder=None):\n","\n","        query = self.fc_q(x)\n","        encoder_key = self.fc_k(encoder_output)\n","        encoder_value = self.fc_v(encoder_output)\n","        query = self.split_heads(query)\n","        encoder_key = self.split_heads(encoder_key, k=True)\n","        encoder_value = self.split_heads(encoder_value)\n","\n","\n","        a = self._attn(query, encoder_key,encoder_value,mask_encoder)\n","        a = self.merge_heads(a)\n","        a = self.c_proj(a)\n","        return a\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n","        super(MLP, self).__init__()\n","        nx = config.n_embd\n","        self.c_fc = Conv1D(n_state, nx)\n","        self.c_proj = Conv1D(nx, n_state)\n","        self.act = gelu\n","\n","    def forward(self, x):\n","        h = self.act(self.c_fc(x))\n","        h2 = self.c_proj(h)\n","        return h2\n","\n","class Block(Module):\n","    def __init__(self, n_ctx, config, scale=False):\n","        super(Block, self).__init__()\n","        nx = config.n_embd\n","\n","        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n","        self.attn = Attention(nx, n_ctx, config, scale,can_be_stateful=True)\n","        self.enc_dec_attn = Enc_Dec_Attention(nx,n_ctx,config,scale)\n","        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n","        self.mlp = MLP(4 * nx, config)\n","        self.resid_pdrop= nn.Dropout(config.resid_pdrop)\n","\n","\n","\n","        self.fc_alpha1 = nn.Linear(nx + nx, nx)\n","        self.fc_alpha2 = nn.Linear(nx + nx, nx)\n","        self.fc_alpha3 = nn.Linear(nx + nx, nx)\n","\n","\n","    def forward(self, x, layer_past=None,mask_queries=None,encoder_output=None,mask_encoder=None, mask_self_attention=None, tau = 0):\n","        threshold = tau\n","\n","        self_attention, present = self.attn(self.ln_1(x), layer_past=layer_past,\n","                                            mask_self_attention=mask_self_attention)\n","        a = x + self_attention\n","        a = self.resid_pdrop(a)\n","\n","\n","        enc_att1 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 0]),mask_encoder=mask_encoder)\n","     \n","        enc_att2 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 1]),mask_encoder=mask_encoder)\n","     \n","        enc_att3 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 2]),mask_encoder=mask_encoder)\n","     \n","\n","        alpha1 = torch.sigmoid(self.fc_alpha1(torch.cat([a, enc_att1], -1)))\n","        alpha2 = torch.sigmoid(self.fc_alpha2(torch.cat([a, enc_att2], -1)))\n","        alpha3 = torch.sigmoid(self.fc_alpha3(torch.cat([a, enc_att3], -1)))\n","\n","\n","        linguistics_alpha1_mask = torch.where(alpha1 > threshold, torch.ones_like(alpha1), torch.zeros_like(alpha1))\n","        linguistics_alpha2_mask = torch.where(alpha2 > threshold, torch.ones_like(alpha2), torch.zeros_like(alpha2))\n","        linguistics_alpha3_mask = torch.where(alpha3 > threshold, torch.ones_like(alpha3), torch.zeros_like(alpha3))\n","\n","\n","        visual_alpha1_mask = torch.where(alpha1 < 1-threshold, torch.ones_like(alpha1), torch.zeros_like(alpha1))\n","        visual_alpha2_mask = torch.where(alpha2 < 1-threshold, torch.ones_like(alpha2), torch.zeros_like(alpha2))\n","        visual_alpha3_mask = torch.where(alpha3 < 1-threshold, torch.ones_like(alpha3), torch.zeros_like(alpha3))\n","\n","\n","\n","        enc_att1 = alpha1* linguistics_alpha1_mask * a + (1-alpha1)* visual_alpha1_mask * enc_att1\n","        enc_att2 = alpha2* linguistics_alpha2_mask * a + (1-alpha2)* visual_alpha2_mask * enc_att2\n","        enc_att3 = alpha3* linguistics_alpha3_mask * a + (1-alpha3)* visual_alpha3_mask* enc_att3\n","\n","        enc_att = (enc_att1 + enc_att2 + enc_att3) / np.sqrt(3)\n","        a = enc_att * mask_queries\n","\n","        m = self.mlp(self.ln_2(a))\n","\n","        encoder_result = a + m\n","\n","        encoder_result = self.resid_pdrop(encoder_result)\n","\n","        encoder_result = encoder_result  * mask_queries\n","        return encoder_result, present\n","\n","class GPT2Model(Module):\n","    def __init__(self, config):\n","        super(GPT2Model, self).__init__()\n","        self.n_layer = config.n_layer\n","        self.n_embd = config.n_embd\n","        self.n_vocab = config.vocab_size\n","\n","        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n","        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n","        block = Block(config.n_ctx, config, scale=True)\n","        self.h = ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n","        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n","        self.register_state('running_seq', torch.zeros((1,)).long())\n","\n","\n","    def set_embeddings_weights(self, model_embeddings_weights):\n","        embed_shape = model_embeddings_weights.shape\n","        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n","        self.decoder.weight = model_embeddings_weights  # Tied weights\n","\n","    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None,mask_queries=None,encoder_output=None,mask_encoder=None, mask_self_attention = None, tau = 0):\n","\n","\n","        if past is None:\n","            past_length = 0\n","            past = [None] * len(self.h)\n","        else:\n","            past_length = past[0][0].size(-2)\n","        if position_ids is None:\n","            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long,\n","                                        device=input_ids.device)\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        input_shape = input_ids.size()\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        position_ids = position_ids.view(-1, position_ids.size(-1))\n","\n","        inputs_embeds = self.wte(input_ids)\n","        position_embeds = self.wpe(position_ids)\n","        if token_type_ids is not None:\n","            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n","            token_type_embeds = self.wte(token_type_ids)\n","        else:\n","            token_type_embeds = 0\n","        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n","        presents = []\n","\n","\n","        for block, layer_past in zip(self.h, past):\n","            hidden_states, present = block(hidden_states, layer_past,mask_queries = mask_queries,encoder_output=encoder_output,mask_encoder=mask_encoder, mask_self_attention= mask_self_attention, tau = tau)\n","            presents.append(present)\n","        hidden_states = self.ln_f(hidden_states)\n","        output_shape = input_shape + (hidden_states.size(-1),)\n","        return hidden_states.view(*output_shape), presents\n","\n","class GPT2LMHead(Module):\n","    def __init__(self, model_embeddings_weights, config):\n","        super(GPT2LMHead, self).__init__()\n","        self.n_embd = config.n_embd\n","        self.set_embeddings_weights(model_embeddings_weights)\n","\n","    def set_embeddings_weights(self, model_embeddings_weights):\n","        embed_shape = model_embeddings_weights.shape\n","        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n","        self.decoder.weight = model_embeddings_weights  # Tied weights\n","\n","    def forward(self, hidden_state):\n","\n","        lm_logits = self.decoder(hidden_state)\n","        return lm_logits\n","\n","\n","\n","class GPT2LMHeadModel(Module):\n","    def __init__(self, config,padding_idx =47932, tau = 0):\n","        super(GPT2LMHeadModel, self).__init__()\n","        self.transformer = GPT2Model(config)\n","        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n","        self.padding_idx = padding_idx\n","\n","        self.register_state('running_mask_self_attention', torch.zeros((1, 1, 0)).bool())\n","        self.tau = tau\n","\n","\n","\n","    def set_tied(self):\n","        \"\"\" Make sure we are sharing the embeddings\n","        \"\"\"\n","        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n","\n","    def forward(self, input_ids, encoder_output=None, mask_encoder=None, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n","\n","        b_s, seq_len = input_ids.shape[:2]\n","        mask_queries = (input_ids != self.padding_idx).unsqueeze(-1).float()\n","\n","        mask_self_attention = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8, device=input_ids.device),\n","                                         diagonal=1)\n","        mask_self_attention = mask_self_attention.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n","        mask_self_attention = mask_self_attention + (input_ids == self.padding_idx).unsqueeze(1).unsqueeze(1).bool()\n","        mask_self_attention = mask_self_attention.gt(0)  # (b_s, 1, seq_len, seq_len)\n","        if self._is_stateful:\n","            self.running_mask_self_attention = torch.cat([self.running_mask_self_attention, mask_self_attention], -1)\n","            mask_self_attention = self.running_mask_self_attention\n","\n","\n","\n","        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past,mask_queries=mask_queries,encoder_output=encoder_output,mask_encoder=mask_encoder, mask_self_attention= mask_self_attention, tau = self.tau)\n","        lm_logits = self.lm_head(hidden_states)\n","        if lm_labels is not None:\n","            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n","            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n","            return loss\n","\n","        lm_logits = F.log_softmax(lm_logits,dim=-1)\n","        return lm_logits, presents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJyxSDtArYEi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wnLG9tOGscMH"},"source":["# Our Model"]},{"cell_type":"code","metadata":{"id":"kpMSbbgPsdPs"},"source":["import torch \n","import torch.nn as nn\n","import copy\n","\n","state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n","\n","class Transformer_visualgpt(Module):\n","  def __init__(self, bos_idx, encoder, padding_idx=47932, n_layer=12, tau=0):\n","    super(Transformer_visualgpt, self).__init__()\n","    self.bos_idx = bos_idx\n","    self.encoder = encoder\n","    config = GPT2Config()\n","    config.n_layer = n_layer\n","    decoder = GPT2Model(config)\n","    decoder = load_weight(decoder, state_dict)\n","    self.decoder = decoder\n","\n","    self.padding_idx = padding_idx\n","    self.tau = tau\n","\n","    self.register_state('enc_output', None)\n","    self.register_state('mask_enc', None)\n","    self.init_weights()\n","  \n","  def init_weights(self):\n","    for p in self.encoder.parameters():\n","      if p.dim()> 1:\n","        nn.init.xavier_uniform_(p)\n","\n","  def forward(self, input_ids, images, *args):\n","    enc_output, mask_enc = self.encoder(images)\n","\n","\n","    b_s, seq_len = input_ids.shape[:2]\n","    mask_queries = (input_ids != self.padding_idx).unsqueeze(-1).float()\n","    mask_self_attention = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8, device=input_ids.device), diagonal=1)\n","    mask_self_attention = mask_self_attention.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n","    mask_self_attention = mask_self_attention + (input_ids == self.padding_idx).unsqueeze(1).unsqueeze(1).bool()\n","    mask_self_attention = mask_self_attention.gt(0)  # (b_s, 1, seq_len, seq_len)\n","    if self._is_stateful:\n","      self.running_mask_self_attention = torch.cat([self.running_mask_self_attention, mask_self_attention], -1)\n","      mask_self_attention = self.running_mask_self_attention\n","    \n","    hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past,mask_queries=mask_queries,encoder_output=encoder_output,mask_encoder=mask_encoder, mask_self_attention= mask_self_attention, tau = self.tau)\n","    \n","    return hidden_states, presents\n","\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLQCUsLRsmqC"},"source":["encoder = VisualEncoder(3, 0, attention_module=ScaledDotProductAttention)\n","model = Transformer_visualgpt(0, encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhRFq_QExyMt","executionInfo":{"status":"ok","timestamp":1618897077153,"user_tz":240,"elapsed":222,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"314437f8-fbfc-474d-e029-b24a360b8986"},"source":["pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total number of trainable parameters:\", pytorch_total_params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of trainable parameters: 239976960\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FmSnXPWfyFaG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-m5ZAB2Wy3JR"},"source":["## Setup tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":667},"id":"n4lcvskvyv1F","executionInfo":{"status":"ok","timestamp":1618897379478,"user_tz":240,"elapsed":126908,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"3ff9e648-9421-4146-eff0-3e7e881fb319"},"source":["!git clone https://github.com/openai/CLIP.git\n","!pip install -e CLIP/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'CLIP' already exists and is not an empty directory.\n","Obtaining file:///content/CLIP\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n","Collecting torch~=1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n","\u001b[?25hCollecting torchvision~=0.8.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 42.5MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.1.2)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=f621f7b21b19f509a6e45e0a2462cf0d7863e22454dfcc457f76f955e89c9398\n","  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n","Successfully built ftfy\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: ftfy, torch, torchvision, clip\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","  Running setup.py develop for clip\n","Successfully installed clip ftfy-6.0.1 torch-1.7.1 torchvision-0.8.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218,"referenced_widgets":["1f70b52d87bd4509a7d7d292c8320c43","bf137f37007f49c5abf3911a3cf0efdd","8a03edb8d0e8436d97b39115c11207e8","9b4d7a4f7d1c41d9965b4f65f4625bf3","3c68d4eb9eed489685b43a1e1beaa101","d7851c73a1e240cbba2b2598e41b188a","189998110fce448c93a4bdef347e8fa7","1d889d8900874afcb4488ff3e0f21d77","d650128970f4444fbf00b1421752cb85","a7477923ec6b43d38563ce62f4910b6d","a1451128d4f44078ae0eb03d8fa6c8e9","79c33af98b8246cc94590a05b3c6479d","c611003150e842818ed58288dbb539bf","4b6897492e59482385cb258bafbbf607","85afdc044fdd4d4fbba8667c0f903bf3","3ab3aad1ca04450dac31428de2fe9153","4f0f7f5e854a4cdbaedc7e64e8e55a6a","9094f9ba4f404d2890eea5aa583292c0","d9a81ede383d466d980efa4bed8a973c","227717146daa4ef4b5c1e88478b21c03","82b083239afc409d8297fca030e06da6","dbb03d3c00d34c51a853ffce81588f75","7ae5a95e15744adc8acf6e796a5c0b7f","313ca38fd3b54379a56bd2d61229e9f0"]},"id":"m70-XqqjzkzT","executionInfo":{"status":"ok","timestamp":1618897748037,"user_tz":240,"elapsed":734,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"393d48c7-0095-4c05-ffb8-f340eaa3c463"},"source":["!pip install transformers\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer(\"Hello world\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f70b52d87bd4509a7d7d292c8320c43","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d650128970f4444fbf00b1421752cb85","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f0f7f5e854a4cdbaedc7e64e8e55a6a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [15496, 995], 'attention_mask': [1, 1]}"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"3_6Q33890vqo"},"source":["\"\"\"\n","Most of the source code is taken from\n","https://www.drivendata.co/blog/hateful-memes-benchmark/\n","\"\"\"\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torch\n","\n","from PIL import Image\n","\n","\n","class HatefulMemesDataset(Dataset):\n","\n","    def __init__(self, jsonl_path, path_to_img_dir,\n","                 image_transform, text_transform):\n","        \"\"\"\n","        :param jsonl_path: path to jsonl provided by Facebook (e.g. data/train.jsonl\n","        :param path_to_img_dir: path to parent directory of img dir\n","        :param image_transform: torchvision.transforms.Compose\n","        :param text_transform: (texts: Union[str, List[str]]) -> torch.LongTensor\n","        \"\"\"\n","        self.samples_frame = pd.read_json(jsonl_path, lines=True)\n","        self.samples_frame = self.samples_frame.reset_index(drop=True)\n","        self.samples_frame.img = self.samples_frame.apply(lambda row: (path_to_img_dir + '/' + row.img), axis=1)\n","        self.image_transform = image_transform\n","        self.text_transform = text_transform\n","\n","    def __len__(self):\n","        return len(self.samples_frame)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        img_id = self.samples_frame.loc[idx, \"id\"]\n","        image = Image.open(self.samples_frame.loc[idx, \"img\"]).convert(\"RGB\")\n","        image = self.image_transform(image)\n","\n","        # TODO: Find a better way for reducing length of a sentence.\n","        # this is an actual sentence from the dataset:\n","        #\n","        # we only want to make you register them, restrict transfers, \n","        # ban certain guns, limit magazine capacity, prohibit carrying them, \n","        # ban or limit ammo, make other arbitrary laws, and, if we catch you \n","        # violating any of these made-up rules, throw you in prison.... at \n","        # which point we will take your guns!\n","\n","        text = self.samples_frame.loc[idx, \"text\"]\n","        text = self.text_transform(text)\n","        # print(text)\n","\n","        # case: development\n","        if \"label\" in self.samples_frame.columns:\n","            label = torch.Tensor(\n","                [self.samples_frame.loc[idx, \"label\"]]\n","            ).long().squeeze()\n","            return image, text, label\n","        else:\n","            # case: inference\n","            return image, text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuUIjYRk1Bik"},"source":["\"\"\"\n","Simple training loop\n","\"\"\"\n","import math\n","import logging\n","from itertools import chain\n","\n","from tqdm.notebook import tqdm\n","import numpy as np\n","\n","import torch\n","\n","from torch.utils.data import DistributedSampler, DataLoader\n","\n","from torch.cuda.amp import autocast\n","from transformers import AdamW\n","\n","class Trainer:\n","    def __init__(self, model, loss_f, image_preprocess, text_preprocess, h, ckpt_path):\n","        \"\"\"\n","        :param model: torch.Module(text, image) -> 0 or 1 (binary classification)\n","        :param loss_f: (model's output, target) -> a real number wrapped by torch.Tensor\n","        :param dictionary that contains the hyper-parameter values\n","        \"\"\"\n","        self.model = model.cuda()\n","        self.loss_f = loss_f\n","        self.h = h\n","        self.image_preprocess = image_preprocess\n","        self.text_preprocess = text_preprocess\n","        self.ckpt_path = ckpt_path\n","\n","    def save_checkpoint(self):\n","        # DataParallel wrappers keep raw model object in .module attribute\n","        raw_model = self.model\n","        print(\"saving\", self.ckpt_path)\n","        torch.save(raw_model.state_dict(), self.ckpt_path)\n","\n","    def train(self, trainset_jsonl, trainset_image_dir_path, valset_jsonl, valset_image_dir_path):\n","        model, loss_f, h = self.model, self.loss_f, self.h\n","        optimizer = AdamW(model.parameters(),lr=1e-4,betas=(0.9, 0.999), eps=1e-8)\n","\n","        train_dataset = HatefulMemesDataset(\n","            trainset_jsonl,\n","            trainset_image_dir_path,\n","            image_transform=self.image_preprocess,\n","            text_transform=self.text_preprocess)\n","\n","        val_dataset = HatefulMemesDataset(\n","            valset_jsonl,\n","            valset_image_dir_path,\n","            image_transform=self.image_preprocess,\n","            text_transform=self.text_preprocess\n","        )\n","\n","        self.device = 'cpu'\n","        if torch.cuda.is_available():\n","            self.device = torch.cuda.current_device()\n","\n","        def run_epoch(split):\n","            is_train = split == 'train'\n","            # Custom code for VisualBERT + CLIP\n","            model.embedding.train()\n","            model.encoder.train()\n","            model.pooler.train()\n","            model.classifier.train()\n","\n","            data = train_dataset if is_train else val_dataset\n","            loader = DataLoader(data, shuffle=True, pin_memory=True,\n","                                batch_size=h[\"batch_size\"],\n","                                num_workers=h[\"num_workers\"])\n","\n","            losses = []\n","            num_correct_pred = 0\n","            num_pred = 0\n","            pbar = tqdm(enumerate(loader), total=len(loader), position=0, leave=True) if is_train else enumerate(loader)\n","            for it, (image, text, label) in pbar:\n","                text = tokenizer(text, return_tensors=\"pt\", padding=True)\n","                # place data on the correct device\n","                text = text.to(self.device)\n","                image = image.to(self.device)\n","                label = label.type(torch.LongTensor).to(self.device)\n","\n","                with torch.set_grad_enabled(is_train):\n","                    with autocast():\n","                        \n","                        output = model(text=text, image=image)\n","                        loss = loss_f(output, label)\n","                        losses.append(loss.item())\n","\n","                if is_train:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                    pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}\")\n","\n","                if not is_train:\n","                    prediction = torch.argmax(output, dim=1)\n","                    num_correct_pred += torch.sum(prediction == label)\n","                    num_pred += prediction.shape[0]\n","\n","            if not is_train:\n","                test_loss = float(np.mean(losses))\n","                acc = num_correct_pred / num_pred\n","                print(\"test loss:\", test_loss)\n","                print(\"test accL\", acc)\n","                return test_loss\n","\n","        best_loss = float('inf')\n","        for epoch in range(h[\"max_epochs\"]):\n","            run_epoch('train')\n","            if val_dataset is not None:\n","                test_loss = run_epoch('test')\n","\n","            # supports early stopping based on the test loss, or just save always if no test set is provided\n","            good_model = val_dataset is None or test_loss < best_loss\n","            if self.ckpt_path is not None and good_model:\n","                best_loss = test_loss\n","                self.save_checkpoint()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wux3EEzt1Jny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618898195528,"user_tz":240,"elapsed":223027,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"3d3902ae-249a-4263-e087-bf2d2626b471"},"source":["# Thank you Mario 🙏\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","!cp '/gdrive/MyDrive/MemesDeepLearning/dataFB.zip' '/content/data.zip'\n","!unzip -q data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"R4z_-npr1qze","executionInfo":{"status":"error","timestamp":1618898274161,"user_tz":240,"elapsed":240,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"7c6dbf26-4ad7-4f6f-9427-4c5bffb95ff9"},"source":["import torch.nn.functional as F \n","\n","\n","model = model\n","loss_f = nn.CrossEntropyLoss()\n","image_preprocess = clip_image_preprocess\n","text_preprocess = lambda x: x\n","h = {\n","  \"batch_size\": 25,\n","  \"num_workers\": 2,\n","  \"max_epochs\": 32,\n","}\n","\n","\n","trainer = Trainer(model, loss_f, image_preprocess, text_preprocess, h, \"experiment_0.pt\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-c2b050ee822d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_preprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_preprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experiment_0.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-56-8863e799564b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, loss_f, image_preprocess, text_preprocess, h, ckpt_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m         r\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mas\u001b[0m \u001b[0mwell\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTypical\u001b[0m \u001b[0muse\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0minitializing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m \u001b[0malso\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n\u001b[1;32m    169\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frlbnzX320Yz","executionInfo":{"status":"ok","timestamp":1618898638702,"user_tz":240,"elapsed":535,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"b4e1810a-37fe-4468-e19f-70500d200bdb"},"source":["clip.encode_image(torch.randn(1, 3, 224, 224)).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"bgN9A4or35Xa"},"source":["# config = GPT2Config()\n","input_ids = tokenizer(\"Hello world\")\n","# GPT2Model(config)(sample[\"input_ids\"], encoder_output=torch.randn(1, 512))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Liddptf6Gxa","executionInfo":{"status":"ok","timestamp":1618899349865,"user_tz":240,"elapsed":545,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"50216e49-fe39-44d7-b351-ad8445ba7e79"},"source":["input_ids = torch.Tensor(input_ids['input_ids']).unsqueeze(0)\n","input_ids"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[15496.,   995.]])"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"QKWwcz3P4ZfU","executionInfo":{"status":"error","timestamp":1618899359384,"user_tz":240,"elapsed":329,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"ea62ad37-005a-47ec-a4d8-1275a7a9843d"},"source":["b_s, seq_len = input_ids.shape[:2]\n","mask_queries = (input_ids != 0).unsqueeze(-1).float()\n","mask_self_attention = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8, device=input_ids.device), diagonal=1)\n","mask_self_attention = mask_self_attention.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n","mask_self_attention = mask_self_attention + (input_ids == 0).unsqueeze(1).unsqueeze(1).bool()\n","mask_self_attention = mask_self_attention.gt(0)  # (b_s, 1, seq_len, seq_len)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-44b0b4bd3bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask_self_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_self_attention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmask_self_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_self_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (b_s, 1, seq_len, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_queries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_queries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_self_attention\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmask_self_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6VxbHv4u6B_g","executionInfo":{"status":"error","timestamp":1618899408734,"user_tz":240,"elapsed":2345,"user":{"displayName":"Tony Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggat1CIAoaXE8FhS0zW0a78QF064OoQp3PAl9k-=s64","userId":"13809858961458950753"}},"outputId":"e9aa25e6-ecd8-4b43-f206-f565764264a8"},"source":["config = GPT2Config()\n","GPT2Model(config)(input_ids.type(torch.LongTensor), encoder_output=torch.randn(1, 512))\n","# tm((input_ids), None, None, None, encoder_output=torch.randn(1, 512))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-108-6226dc3a17fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tm((input_ids), None, None, None, encoder_output=torch.randn(1, 512))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mBoth\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mKeys\u001b[0m \u001b[0mare\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-ffc8253a94b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, token_type_ids, past, mask_queries, encoder_output, mask_encoder, mask_self_attention, tau)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_queries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_self_attention\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmask_self_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mpresents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mBoth\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mKeys\u001b[0m \u001b[0mare\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-ffc8253a94b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, mask_queries, encoder_output, mask_encoder, mask_self_attention, tau)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0menc_att1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dec_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0menc_att2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dec_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mBoth\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpersistent\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mKeys\u001b[0m \u001b[0mare\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-ffc8253a94b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, encoder_output, mask_encoder)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mencoder_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mencoder_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mencoder_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-ffc8253a94b0>\u001b[0m in \u001b[0;36msplit_heads\u001b[0;34m(self, x, k)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# in Tensorflow implem: fct split_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, head, head_features, seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, head, seq_length, head_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"]}]},{"cell_type":"code","metadata":{"id":"_dqOzrbZ60-1"},"source":[""],"execution_count":null,"outputs":[]}]}