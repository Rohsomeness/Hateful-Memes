{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MMBT_inference.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9X5EXS1g5ONwO0oiQn/yz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3MpQfTLqmZ33"},"source":["# Thank you Mario ðŸ™\n","from google.colab import drive\n","drive.mount('/gdrive')\n","#drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji83oKBsmsN7"},"source":["!cp '/gdrive/MyDrive/MemesDeepLearning/dataFB.zip' '/content/data.zip'\n","!unzip -q data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFrnJCsalD4I"},"source":["!cp '/gdrive/MyDrive/MemesDeepLearning/test.jsonl' '/content/test.jsonl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7J9HZedlD-O"},"source":["!cp '/gdrive/MyDrive/MemesDeepLearning/mmbt_model_best.zip' '/content/best_model.zip'\n","!unzip -q best_model.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Hgj_nO7msTF"},"source":["!pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5ymZ1OjmsYc"},"source":["!pip install sklearn pytorch-pretrained-bert numpy tqdm matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6GSCGAgyrAz"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from pytorch_pretrained_bert.modeling import BertModel\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from pytorch_pretrained_bert import BertTokenizer\n","import json\n","import numpy as np\n","import os\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","import functools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBJP5W1iHEBz"},"source":["class Vocab(object):\n","    def __init__(self, emptyInit=False):\n","        if emptyInit:\n","            self.stoi, self.itos, self.vocab_sz = {}, [], 0\n","        else:\n","            self.stoi = {\n","                w: i\n","                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","            }\n","            self.itos = [w for w in self.stoi]\n","            self.vocab_sz = len(self.itos)\n","\n","    def add(self, words):\n","        cnt = len(self.itos)\n","        for w in words:\n","            if w in self.stoi:\n","                continue\n","            self.stoi[w] = cnt\n","            self.itos.append(w)\n","            cnt += 1\n","        self.vocab_sz = len(self.itos)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6JBtfGExbuV"},"source":["class ImageBertEmbeddings(nn.Module):\n","    def __init__(self, embeddings, vocab):\n","        super(ImageBertEmbeddings, self).__init__()\n","        self.img_embeddings = nn.Linear(2048, 768)\n","        self.position_embeddings = embeddings.position_embeddings\n","        self.token_type_embeddings = embeddings.token_type_embeddings\n","        self.word_embeddings = embeddings.word_embeddings\n","        self.LayerNorm = embeddings.LayerNorm\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.vocab = vocab\n","\n","    def forward(self, input_imgs, token_type_ids):\n","        bsz = input_imgs.size(0)\n","        seq_length = 3 + 2  # +2 for CLS and SEP Token\n","\n","        cls_id = torch.LongTensor([self.vocab.stoi[\"[CLS]\"]]).cuda()\n","        cls_id = cls_id.unsqueeze(0).expand(bsz, 1).cuda()\n","        cls_token_embeds = self.word_embeddings(cls_id).cuda()\n","\n","        sep_id = torch.LongTensor([self.vocab.stoi[\"[SEP]\"]]).cuda()\n","        sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n","        sep_token_embeds = self.word_embeddings(sep_id)\n","\n","        imgs_embeddings = self.img_embeddings(input_imgs)\n","        token_embeddings = torch.cat(\n","            [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1\n","        )\n","\n","        position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n","        position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class MultimodalBertEncoder(nn.Module):\n","    def __init__(self, vocab):\n","        super(MultimodalBertEncoder, self).__init__()\n","        bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","        self.txt_embeddings = bert.embeddings\n","        self.vocab = vocab\n","        self.img_embeddings = ImageBertEmbeddings(self.txt_embeddings, self.vocab)\n","        self.img_encoder = ImageEncoder()\n","        self.encoder = bert.encoder\n","        self.pooler = bert.pooler\n","        self.clf = nn.Linear(768, 2)\n","\n","    def forward(self, input_txt, attention_mask, segment, input_img):\n","        bsz = input_txt.size(0)\n","\n","        attention_mask = torch.cat(\n","            [\n","                torch.ones(bsz, 3 + 2).long().cuda(),\n","                attention_mask,#.cuda(),\n","            ],\n","            dim=1,\n","        )\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        img_tok = (\n","            torch.LongTensor(input_txt.size(0), 3 + 2)\n","            .fill_(0)\n","            .cuda()\n","        )\n","        img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n","        img_embed_out = self.img_embeddings(img, img_tok)\n","        txt_embed_out = self.txt_embeddings(input_txt, segment)\n","        encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n","\n","        encoded_layers = self.encoder(\n","            encoder_input, extended_attention_mask, output_all_encoded_layers=False\n","        )\n","\n","        return self.pooler(encoded_layers[-1])\n","\n","\n","class MultimodalBertClf(nn.Module):\n","    def __init__(self, vocab):\n","        super(MultimodalBertClf, self).__init__()\n","        self.enc = MultimodalBertEncoder(vocab)\n","        self.clf = nn.Linear(768, 2)\n","\n","    def forward(self, txt, mask, segment, img):\n","        x = self.enc(txt, mask, segment, img)\n","        return self.clf(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuNBKJ4Qyl8c"},"source":["class ImageEncoder(nn.Module):\n","    def __init__(self):\n","        super(ImageEncoder, self).__init__()\n","        model = torchvision.models.resnet152(pretrained=True)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","\n","        pool_func = nn.AdaptiveAvgPool2d\n","        self.pool = pool_func((3, 1))\n","\n","    def forward(self, x):\n","        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n","        out = self.pool(self.model(x))\n","        out = torch.flatten(out, start_dim=2)\n","        out = out.transpose(1, 2).contiguous()\n","        return out  # BxNx2048\n","\n","\n","class ImageClf(nn.Module):\n","    def __init__(self):\n","        super(ImageClf, self).__init__()\n","        self.img_encoder = ImageEncoder()\n","        self.clf = nn.Linear(2048 * 3, 2)\n","\n","    def forward(self, x):\n","        x = self.img_encoder(x)\n","        x = torch.flatten(x, start_dim=1)\n","        out = self.clf(x)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"af26GrEvIL1M"},"source":["def get_labels_and_frequencies(path):\n","    label_freqs = Counter()\n","    data_labels = [json.loads(line)[\"label\"] for line in open(path)]\n","    if type(data_labels[0]) == list:\n","        for label_row in data_labels:\n","            label_freqs.update(label_row)\n","    else:\n","        label_freqs.update(data_labels)\n","\n","    return list(label_freqs.keys()), label_freqs\n","\n","def collate_fn(batch):\n","    lens = [len(row[0]) for row in batch]\n","    bsz, max_seq_len = len(batch), max(lens)\n","\n","    mask_tensor = torch.zeros(bsz, max_seq_len).long()\n","    text_tensor = torch.zeros(bsz, max_seq_len).long()\n","    segment_tensor = torch.zeros(bsz, max_seq_len).long()\n","\n","    img_tensor = None\n","    img_tensor = torch.stack([row[2] for row in batch])\n","\n","    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n","        tokens, segment = input_row[:2]\n","        text_tensor[i_batch, :length] = tokens\n","        segment_tensor[i_batch, :length] = segment\n","        mask_tensor[i_batch, :length] = 1\n","\n","    return text_tensor, segment_tensor, mask_tensor, img_tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvdoE0qP9kb3"},"source":["import json\n","import numpy as np\n","import os\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","class JsonlDataset(Dataset):\n","    def __init__(self, data_path, tokenizer, transforms, vocab, train):\n","        self.data = [json.loads(l) for l in open(data_path)]\n","        self.data_dir = os.path.dirname(data_path)\n","        self.tokenizer = tokenizer\n","        self.vocab = vocab\n","        self.n_classes = 2\n","        self.text_start_token = [\"[SEP]\"]\n","        self.train = train\n","\n","        self.max_seq_len = 512\n","        self.max_seq_len -= 3\n","\n","        self.transforms = transforms\n","\n","        if train:\n","            self.labels = get_labels_and_frequencies(data_path)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        sentence = (\n","            self.text_start_token\n","            + self.tokenizer(self.data[index][\"text\"])[\n","                : (512 - 1)\n","            ]\n","        )\n","        segment = torch.zeros(len(sentence))\n","\n","        sentence = torch.LongTensor(\n","            [\n","                self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n","                for w in sentence\n","            ]\n","        )\n","\n","        if self.train:\n","            label = torch.LongTensor(\n","                [self.labels.index(self.data[index][\"label\"])]\n","            )\n","\n","        image = None\n","\n","        if self.data[index][\"img\"]:\n","            image = Image.open(\n","                os.path.join(self.data_dir, self.data[index][\"img\"])\n","            ).convert(\"RGB\")\n","        else:\n","            image = Image.fromarray(128 * np.ones((256, 256, 3), dtype=np.uint8))\n","        image = self.transforms(image)\n","\n","        # The first SEP is part of Image Token.\n","        segment = segment[1:]\n","        sentence = sentence[1:]\n","        # The first segment (0) is of images.\n","        segment += 1\n","\n","        if self.train:\n","            return sentence, segment, image, label\n","        else:\n","            return sentence, segment, image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"257DEOdC8iIm"},"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True).tokenize\n","transform = transforms.Compose(\n","        [\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                mean=[0.46777044, 0.44531429, 0.40661017],\n","                std=[0.12221994, 0.12145835, 0.14380469],\n","            ),\n","        ]\n","    )\n","vocab = Vocab()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1jc_K_jx1Hm"},"source":["model = MultimodalBertClf(vocab)\n","best_checkpoint = torch.load('./model_best.pt', map_location=torch.device('cpu'))\n","model.load_state_dict(best_checkpoint[\"state_dict\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZPmZOeH3RZw"},"source":["model.eval()\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Np-DqPV1b2vJ"},"source":["collate = functools.partial(collate_fn)\n","\n","test_set = JsonlDataset(\n","    \"./data/test.jsonl\",\n","    tokenizer,\n","    transform,\n","    vocab,\n","    False,\n",")\n","\n","test_loader = DataLoader(\n","    test_set,\n","    batch_size=128,\n","    shuffle=False,\n","    num_workers=1,\n","    collate_fn=collate,\n",")\n","\n","val_set = JsonlDataset(\n","    \"./data/dev_seen.jsonl\",\n","    tokenizer,\n","    transform,\n","    vocab,\n","    False,\n",")\n","\n","val_loader = DataLoader(\n","    test_set,\n","    batch_size=128,\n","    shuffle=False,\n","    num_workers=1,\n","    collate_fn=collate,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wf6tYyIifk-8"},"source":["import json\n","import torch\n","\n","resultTest = [json.loads(jline) for jline in open('/content/data/test.jsonl', 'r')]\n","print(len(resultTest))\n","resultTensorTest = torch.zeros(len(resultTest))\n","for i in range(len(resultTest)):\n","  resultTensorTest[i] = resultTest[i]['label']\n","print(resultTensorTest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lv2b0aVDfmdC"},"source":["import json\n","import torch\n","\n","resultVal = [json.loads(jline) for jline in open('/content/data/dev_seen.jsonl', 'r')]\n","print(len(resultVal))\n","resultTensorVal = torch.zeros(len(resultVal))\n","for i in range(len(resultVal)):\n","  resultTensorVal[i] = resultVal[i]['label']\n","print(resultTensorVal)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rKwDRf5OyZp"},"source":["from sklearn import metrics\n","\n","preds, probs = [], []\n","for batch in test_loader:\n","    with torch.no_grad():\n","        txt, segment, mask, img = batch\n","        txt, img = txt.cuda(), img.cuda()\n","        mask, segment = mask.cuda(), segment.cuda()\n","        out = model(txt, mask, segment, img)\n","\n","\n","        prob = torch.nn.functional.softmax(out, dim=1)[:, 1].cpu().detach().numpy()\n","        pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n","\n","        probs.append(prob)\n","        preds.append(pred)\n","\n","preds = np.concatenate(preds, axis=0)\n","probs = np.concatenate(probs, axis=0)\n","\n","predictions = torch.from_numpy(preds)\n","\n","numCorrect = (predictions == resultTensorTest).sum().item()\n","acc = (100.0 * numCorrect / resultTensorTest.shape[0])\n","\n","fpr, tpr, thresholds = metrics.roc_curve(resultTensorTest, probs, pos_label=1)\n","auroc = metrics.auc(fpr, tpr)\n","\n","print(f\"Accuracy: {acc} AUROC: {auroc}\")\n","\n","with open(\"submission1.csv\", \"w\") as f:\n","    f.write(\"proba,label\\n\")\n","    for i in range(preds.shape[0]):\n","        f.write(f\"{probs[i]},{preds[i]}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5dUHr3DhCDD"},"source":["preds, probs = [], []\n","for batch in val_loader:\n","    with torch.no_grad():\n","        txt, segment, mask, img = batch\n","        txt, img = txt.cuda(), img.cuda()\n","        mask, segment = mask.cuda(), segment.cuda()\n","        out = model(txt, mask, segment, img)\n","\n","\n","        prob = torch.nn.functional.softmax(out, dim=1)[:, 1].cpu().detach().numpy()\n","        pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n","\n","        probs.append(prob)\n","        preds.append(pred)\n","\n","preds = np.concatenate(preds, axis=0)\n","probs = np.concatenate(probs, axis=0)\n","\n","predictions = torch.from_numpy(preds)\n","\n","numCorrect = (predictions == resultTensorTest).sum().item()\n","acc = (100.0 * numCorrect / resultTensorTest.shape[0])\n","\n","fpr, tpr, thresholds = metrics.roc_curve(resultTensorTest, probs, pos_label=1)\n","auroc = metrics.auc(fpr, tpr)\n","\n","print(f\"Accuracy: {acc} AUROC: {auroc}\")\n","\n","with open(\"submission2.csv\", \"w\") as f:\n","    f.write(\"proba,label\\n\")\n","    for i in range(preds.shape[0]):\n","        f.write(f\"{probs[i]},{preds[i]}\\n\")"],"execution_count":null,"outputs":[]}]}