{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveCLIPandEmbedder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohsomeness/Hateful-Memes/blob/main/models/multimodal/NaiveCLIPandEmbedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuCX1VOFgPVQ"
      },
      "source": [
        "-Mario Goranov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ujksFZBe03v"
      },
      "source": [
        "# Initialize and copy data zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj0hs93OvyUp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6fHcddTvyU1"
      },
      "source": [
        "!ls '/gdrive/MyDrive/MemesDeepLearning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64x5ihbAgKnl"
      },
      "source": [
        "Zip of original data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QshatkX4C0sR"
      },
      "source": [
        "!cp '/gdrive/MyDrive/MemesDeepLearning/dataFB.zip' '/content/data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh8HVaeygMWO"
      },
      "source": [
        "Zip of preprocessed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU2K96YRt_f7"
      },
      "source": [
        "!cp '/gdrive/MyDrive/MemesDeepLearning/preprocessed_data/img_clean.zip' '/content/img_clean.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecOoEf2FDTVZ"
      },
      "source": [
        "!unzip -q data.zip\n",
        "!unzip -q img_clean.zip\n",
        "!mv img_clean img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHXckZ-ewHe"
      },
      "source": [
        "# Scan train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uQKcyUezjjj"
      },
      "source": [
        "import json\n",
        "\n",
        "result = [json.loads(jline) for jline in open('data/train.jsonl', 'r')]\n",
        "resultDev = [json.loads(jline) for jline in open('data/dev_seen.jsonl', 'r')]\n",
        "resultTest = [json.loads(jline) for jline in open('test.jsonl', 'r')]\n",
        "#print(len(resultTrain))\n",
        "print(len(result))\n",
        "print(len(resultDev))\n",
        "result = resultDev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yQDvS53gWQv"
      },
      "source": [
        "Make sure it displays an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPWfYd9U0GsM"
      },
      "source": [
        "index = 0\n",
        "\n",
        "from IPython.display import Image\n",
        "print(result[index])\n",
        "#print('/gdrive/MyDrive/data/'+result[index]['img'])\n",
        "#if result[index]['label'] is 0:\n",
        "#  print(\"Good meme:\")\n",
        "#else:\n",
        "#  print(\"Hateful meme:\")\n",
        "display(Image('data/'+result[index]['img']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnpk2EmOgbDF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PjGM8EuNJuc"
      },
      "source": [
        "tot = 0\n",
        "for i in range(len(result)):\n",
        "  tot += result[i]['label']\n",
        "print(f\"Share of dataset that is hateful {tot/len(result)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5uklPQuV3kR"
      },
      "source": [
        "# CLIP portions below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GCKsDAEY3B"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "MODELS = {\n",
        "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
        "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n",
        "}\n",
        "!wget {MODELS[\"ViT-B/32\"]} -O model.pt\n",
        "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dwBCSZ0V_gK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "\n",
        "!pip install ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDhyvySFL5H8"
      },
      "source": [
        "Remember to run this code too:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNciCOOUWOcZ",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvEITEdzWRSz"
      },
      "source": [
        "# Run code below as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r802Xp_cecq7"
      },
      "source": [
        "Helper functions, run if necessary but unneeded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxzM4BLBhPj"
      },
      "source": [
        "result = resultDev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fDN_ycB5CYJ"
      },
      "source": [
        "!mkdir newData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEONPtHz9YwY"
      },
      "source": [
        "!rm newData/dev*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPY_Kv2em2g"
      },
      "source": [
        "Either generate embeddings or run naive model. Embeddings were generated into 8 split files\n",
        "\n",
        "Naive: storeAnything=False oldStyle=True\n",
        "\n",
        "Embeddings for train/validation: storeAnything=True storeTruth=True oldStyle=False\n",
        "\n",
        "Embeddings for unlabeled test: storeAnything=True storeTruth=False oldStyle=False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO7iBAplWiQ1"
      },
      "source": [
        "\n",
        "for indiv in range(0,1): #set to range(0,8) to export embeddings\n",
        "  #Change as necessary\n",
        "  showImages = False\n",
        "  storeTruth = True\n",
        "  storeAnything = False\n",
        "  oldStyle = True #checks similarity of descriptions, ignores text\n",
        "  useCleanImages = False #uses preprocessed images\n",
        "\n",
        "  descriptions = {\n",
        "      'good meme': 'a nonhateful meme that is good',\n",
        "      'hateful meme': 'a hateful meme containing racism or sexism'\n",
        "  }\n",
        "  indexes = None\n",
        "  if oldStyle:\n",
        "    indexes = range(len(result))\n",
        "  else:\n",
        "    indexes = range((indiv)*(len(result)//8),(indiv+1)*(len(result)//8))\n",
        "  import os\n",
        "  import skimage\n",
        "  import IPython.display\n",
        "  import matplotlib.pyplot as plt\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  from scipy.special import softmax\n",
        "  from sklearn import metrics\n",
        "  import sklearn\n",
        "  from collections import OrderedDict\n",
        "  import torch\n",
        "\n",
        "  %matplotlib inline\n",
        "  %config InlineBackend.figure_format = 'retina'\n",
        "  images = []\n",
        "\n",
        "  texts = []\n",
        "  if oldStyle:\n",
        "      texts = [descriptions['good meme'],descriptions['hateful meme']]\n",
        " \n",
        "\n",
        "  \n",
        "  groundTruth = np.zeros(len(indexes),dtype=np.uint8)\n",
        "\n",
        "  if showImages:\n",
        "    plt.figure(figsize=(16, 16))\n",
        "\n",
        "\n",
        "  #'data/'+result[index]['img']\n",
        "  i = 0\n",
        "  for index in indexes:\n",
        "      if useCleanImages:\n",
        "        filepath = result[index]['img']\n",
        "      else:\n",
        "        filepath = 'data/'+result[index]['img']\n",
        "      #\n",
        "      if storeTruth:\n",
        "        name = \"good meme\" if result[index]['label']==0 else \"hateful meme\"\n",
        "        groundTruth[i] = result[index]['label']\n",
        "      i+=1\n",
        "      image = preprocess(Image.open(filepath).convert(\"RGB\"))\n",
        "      images.append(image)\n",
        "      if not oldStyle:\n",
        "        texts.append(result[index]['text'])\n",
        "      #texts.append(descriptions[name])\n",
        "      if showImages:\n",
        "        plt.subplot(5, 2, len(images))\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(f\"{filepath} – {result[index]['text']}\\n{descriptions[name]}\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "  if showImages:\n",
        "    plt.tight_layout()\n",
        "\n",
        "  image_input = torch.tensor(np.stack(images)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "  tokenizer = SimpleTokenizer()\n",
        "  text_tokens = [tokenizer.encode(desc) for desc in texts]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "  eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      tokens = tokens[0:73]\n",
        "      tokens = [sot_token] + tokens + [eot_token]\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  text_input = text_input.cuda()\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "  if showImages:\n",
        "    count = len(descriptions)\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "    # plt.colorbar()\n",
        "    plt.yticks(range(count), texts, fontsize=18)\n",
        "    plt.xticks([])\n",
        "    for i, image in enumerate(images):\n",
        "        plt.imshow(image.permute(1, 2, 0), extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "    for x in range(similarity.shape[1]):\n",
        "        for y in range(similarity.shape[0]):\n",
        "            plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "      plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "    plt.xlim([-0.5, count - 0.5])\n",
        "    plt.ylim([count + 0.5, -2])\n",
        "\n",
        "    plt.title(\"Cosine similarity between text and image features\", size=20)\n",
        "\n",
        "\n",
        "  simMax = np.argmax(similarity, axis=0)\n",
        "  #print(similarity)\n",
        "  #print(simMax)\n",
        "  #print(groundTruth)\n",
        "  if storeTruth:\n",
        "    print(f\"ratio groundTruth that is hateful: {np.sum(groundTruth)/groundTruth.shape[0]}\")\n",
        "    print(f\"Accuracy is: {np.sum(simMax==groundTruth)/groundTruth.shape[0]}\")\n",
        "    similarity = np.transpose(similarity)\n",
        "    second = softmax(similarity,axis=1)\n",
        "    #print(second)\n",
        "    #print(second[0])\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(groundTruth, second[:,1], pos_label=1)\n",
        "    auroc = metrics.auc(tpr, fpr)\n",
        "    plt.scatter(fpr,tpr)\n",
        "    print(f\"AUROC is: {sklearn.metrics.roc_auc_score(groundTruth, second[:,1])}\")\n",
        "  if storeAnything:\n",
        "    torch.save(image_features, f'newData/dev_img_{indiv}.pt')\n",
        "    torch.save(text_features, f'newData/dev_txt_{indiv}.pt')\n",
        "    if storeTruth:\n",
        "      torch.save(groundTruth, f'newData/dev_truth_{indiv}.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02xEKU6gnSj"
      },
      "source": [
        "Save the encoded data into a zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dPNslI2M5y"
      },
      "source": [
        "!zip tensorDataPreprocessed.zip newData/*"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}